# ğŸ¤– Extraer Respuesta del LLM (OpenAI)

## ğŸ¯ Objetivo

Extraer el mensaje generado por OpenAI desde:
```
workflow.openia.choices[0].content
```

Y guardarlo en:
```
workflow.aiResponse
```

---

## ğŸš€ Uso en Botpress

### OpciÃ³n 1: VersiÃ³n Completa (Recomendada)

**Archivo:** `botpress-extract-ai-response.js`

âœ… Manejo completo de errores
âœ… Logs detallados para debugging
âœ… ValidaciÃ³n de estructura
âœ… Mensajes de fallback

**CuÃ¡ndo usar:**
- ProducciÃ³n
- Necesitas debugging
- Quieres logs detallados

### OpciÃ³n 2: VersiÃ³n Simple

**Archivo:** `botpress-extract-ai-simple.js`

âœ… Solo 2 lÃ­neas de cÃ³digo
âœ… MÃ­nimo overhead
âœ… MÃ¡s rÃ¡pido

**CuÃ¡ndo usar:**
- Testing rÃ¡pido
- Prototipo
- Ya confÃ­as en que funciona

---

## ğŸ“‹ ImplementaciÃ³n

### Paso 1: Ubicar el Nodo

DespuÃ©s de tu **AI Task / Generate (OpenAI)**, agrega un nodo de **Execute Code**.

### Paso 2: Copiar el CÃ³digo

**VersiÃ³n Completa:**
```javascript
try {
  console.log('ğŸ¤– Extrayendo respuesta del LLM...');

  if (workflow.openia && 
      workflow.openia.choices && 
      Array.isArray(workflow.openia.choices) && 
      workflow.openia.choices.length > 0) {
    
    const aiMessage = workflow.openia.choices[0].content;
    workflow.aiResponse = aiMessage;
    
    console.log('âœ… Respuesta del AI extraÃ­da exitosamente');
    console.log('ğŸ“ Longitud:', aiMessage ? aiMessage.length : 0, 'caracteres');
    console.log('ğŸ¤– Model:', workflow.openia.model || 'Unknown');
    
  } else {
    console.error('âŒ Error: workflow.openia no tiene la estructura esperada');
    workflow.aiResponse = 'Lo siento, hubo un error al generar la respuesta.';
  }
  
} catch (error) {
  console.error('âŒ Error:', error.message);
  workflow.aiResponse = 'Lo siento, hubo un error tÃ©cnico.';
}

console.log('âœ… workflow.aiResponse:', workflow.aiResponse ? 'Asignado âœ“' : 'VacÃ­o âœ—');
```

**VersiÃ³n Simple:**
```javascript
workflow.aiResponse = workflow.openia?.choices?.[0]?.content || 'Error al generar respuesta';
console.log('âœ… AI Response:', workflow.aiResponse ? 'Guardado' : 'Error');
```

### Paso 3: Usar la Variable

Ahora puedes usar `workflow.aiResponse` en cualquier lugar:

**En un nodo de texto:**
```
{{workflow.aiResponse}}
```

**En cÃ³digo JavaScript:**
```javascript
const respuesta = workflow.aiResponse;
console.log(respuesta);
```

**En condiciones:**
```javascript
if (workflow.aiResponse && workflow.aiResponse.length > 0) {
  // Hay respuesta
} else {
  // No hay respuesta
}
```

---

## ğŸ” Estructura de workflow.openia

```javascript
workflow.openia = {
  id: "chatcmpl-CUmjP1jErrR58eSH7NQQZPMa2R53w",
  provider: "OpenAI",
  model: "gpt-5-mini-2025-08-07",
  choices: [
    {
      role: "assistant",
      type: "text",
      content: "Â¡Hola! Soy un asistente experto en productos..." // â† ESTO extraemos
    }
  ]
}
```

**Extraemos:** `choices[0].content` â† El mensaje del AI

**Guardamos en:** `workflow.aiResponse`

---

## ğŸ“Š Ejemplo Completo

```javascript
// Antes de ejecutar el cÃ³digo
workflow.openia = { 
  choices: [
    { 
      content: "Â¡Hola Marcelo! Con tu crÃ©dito de 730 puntos..." 
    }
  ] 
};

// DespuÃ©s de ejecutar el cÃ³digo
workflow.aiResponse = "Â¡Hola Marcelo! Con tu crÃ©dito de 730 puntos...";

// Ahora puedes usar workflow.aiResponse en cualquier parte
```

---

## ğŸ”§ Debugging

### Ver los Logs

En Botpress Console verÃ¡s:

**VersiÃ³n Completa:**
```
ğŸ¤– Extrayendo respuesta del LLM...
âœ… Respuesta del AI extraÃ­da exitosamente
ğŸ“ Longitud del mensaje: 487 caracteres
ğŸ¯ Provider: OpenAI
ğŸ¤– Model: gpt-5-mini-2025-08-07
ğŸ’¬ Preview: Â¡Hola Marcelo! Con tu crÃ©dito de 730 puntos...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… EXTRACCIÃ“N COMPLETADA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
workflow.aiResponse: Asignado âœ“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**VersiÃ³n Simple:**
```
âœ… AI Response: Guardado
```

### Si hay Error

```
âŒ Error: workflow.openia no tiene la estructura esperada
Estructura actual: {...}
workflow.aiResponse: Error al generar respuesta
```

---

## âš ï¸ Notas Importantes

1. **Ejecuta DESPUÃ‰S del AI Task**
   - El cÃ³digo debe ejecutarse DESPUÃ‰S de que OpenAI genere la respuesta

2. **workflow.openia debe existir**
   - Si no existe, asignarÃ¡ mensaje de error

3. **Fallback automÃ¡tico**
   - Si falla, `workflow.aiResponse` tendrÃ¡ un mensaje de error apropiado

4. **Compatible con optional chaining**
   - La versiÃ³n simple usa `?.` para evitar errores si algo no existe

---

## ğŸ¯ Flujo Completo

```
1. Usuario envÃ­a mensaje
   â†“
2. Hook carga contexto (user.aiPrompt)
   â†“
3. AI Task genera respuesta (workflow.openia)
   â†“
4. Execute Code extrae mensaje (workflow.aiResponse)
   â†“
5. Bot envÃ­a workflow.aiResponse al usuario
```

---

## âœ… Checklist

- [ ] Hook configurado (carga user.aiPrompt)
- [ ] AI Task configurado (usa {{user.aiPrompt}})
- [ ] Execute Code agregado DESPUÃ‰S del AI Task
- [ ] CÃ³digo copiado (versiÃ³n completa o simple)
- [ ] workflow.aiResponse usado en siguiente nodo
- [ ] Probado con mensaje de prueba

---

Â¡Listo! Ahora puedes extraer y usar la respuesta del LLM fÃ¡cilmente. ğŸš€

